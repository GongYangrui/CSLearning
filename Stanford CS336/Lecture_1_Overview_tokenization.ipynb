{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Examples\n",
    "é€šè¿‡è¿™ä¸ªç½‘ç«™[Tiktokenizer](https://tiktokenizer.vercel.app/?encoder=gpt2)å¯ä»¥æ„Ÿå—ä¸€ä¸‹åˆ†è¯å™¨æ˜¯å¦‚ä½•è¿ä½œçš„ã€‚\n",
    "```\n",
    "<|im_start|>system<|im_sep|>You are a helpful assistant<|im_end|>\n",
    "<|im_start|>user<|im_sep|>I am studying CS336<|im_end|><|im_start|>assistant<|im_sep|>\n",
    "```\n",
    "è¿™ä¸€æ®µå†…å®¹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œæ˜¯æˆ‘ä»¬å’Œäººå·¥æ™ºèƒ½ä¹‹é—´çš„å¯¹è¯çš„ç³»ç»Ÿæ ¼å¼ï¼Œæˆ‘å‘ç°ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼Œå¦‚æœç›´æ¥å°†ä¸Šé¢è¿™æ®µå†…å®¹å¤åˆ¶ç²˜è´´åˆ°ChatGPTä¸­ï¼Œå¹¶ä¸”è®©å®ƒç»™ä½ è§£é‡Šä¸€ä¸‹ï¼Œå®ƒåªä¼šè®¤ä¸ºä½ æ˜¯ä¸€ä¸ªæ­£åœ¨å­¦ä¹ CS336çš„åŒå­¦ï¼Œä½†æ˜¯å¦‚æœåœ¨å¤–é¢åŠ ä¸€ä¸ªå¼•å·ç„¶åå¼•ä¸Šè¿™æ®µå†…å®¹ï¼ŒChatGPTæ‰èƒ½ä¸ºä½ è§£é‡Šè¿™æ®µå†…å®¹ã€‚è¿™ä¹Ÿæ­£å¥½éªŒè¯äº†è¿™æ˜¯äººç±»å’Œäººå·¥æ™ºèƒ½ä¹‹é—´çš„å¯¹è¯çš„ä¾‹å­ã€‚\n",
    "\n",
    "é€šè¿‡è¿™ä¸ªä¾‹å­è§‚å¯Ÿåˆ°çš„**ç°è±¡**ï¼š\n",
    "- åœ¨è‹±æ–‡ä¸­ä¸€ä¸ªå•è¯å‰é¢çš„ç©ºæ ¼ä¼šå’Œå®ƒä¸€èµ·ç»„æˆä¸€ä¸ªtokenï¼›\n",
    "- ç›¸åŒçš„å•è¯å‡ºç°åœ¨å¥é¦–å’Œå¥ä¸­ï¼Œtokenä¸ä¸€å®šç›¸åŒï¼Œä¾‹å¦‚\"hello\"æ˜¯å¥é¦–çš„tokenï¼Œ\" hello\"æ˜¯å¥ä¸­tokenï¼ˆæ³¨æ„æœ‰ä¸ªç©ºæ ¼ï¼‰ï¼Œè¿™ä¹Ÿå°±ä¼šå¯¼è‡´æœ€åå¾—åˆ°çš„æ•´æ•°ä¸ä¸€æ ·ï¼›\n",
    "- ä¸€ä¸ªæ•°å­—ä¸ä¼šè¢«å½“ä½œä¸€ä¸ªæ•´ä½“ï¼Œä¾‹å¦‚2025å¯èƒ½è¢«åˆ†ä¸ºä¸¤ç»„æ¥ç†è§£ï¼ˆ202ã€5ï¼‰\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/gyr04/anaconda3/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/gyr04/anaconda3/lib/python3.10/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/gyr04/anaconda3/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/gyr04/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gyr04/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/gyr04/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gyr04/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [15496, 11, 12520, 234, 235, 0, 220, 19526, 254, 25001, 121, 0]\n",
      "Decoded Text: Hello, ğŸŒ! ä½ å¥½!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"Hello, ğŸŒ! ä½ å¥½!\"\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**compression ratio**æ˜¯å‹ç¼©ç‡ï¼ŒæŒ‡çš„æ˜¯åŸå§‹çš„å­—ç¬¦ä¸²é•¿åº¦ï¼ˆä»¥UTF-8ç¼–ç ï¼‰ä¸åˆ†è¯åçš„tokenæ•°é‡çš„æ¯”å€¼ã€‚\n",
    "- æ¯”å€¼è¶Šå¤§ä»£è¡¨æ¯ä¸ªtokenèƒ½ä»£è¡¨çš„å­—èŠ‚è¶Šå¤šï¼Œä¹Ÿå°±æ˜¯å‹ç¼©æ•ˆæœå¥½ï¼›\n",
    "- æ¯”å€¼è¶Šå°ä»£è¡¨åˆ†è¯æ›´ç»†ç¢ï¼Œå‹ç¼©æ•ˆæœå·®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6666666666666667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_compression_ratio(string: str, indices: list[int]) -> float:\n",
    "    \"\"\"Given `string` that has been tokenized into `indices`, .\"\"\"\n",
    "    num_bytes = len(bytes(string, encoding=\"utf-8\"))  # @inspect num_bytes\n",
    "    num_tokens = len(indices)                       # @inspect num_tokens\n",
    "    return num_bytes / num_tokens\n",
    "get_compression_ratio(text, token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-based tokenization\n",
    "å­—ç¬¦çº§åˆ†è¯å™¨ä¹Ÿå°±æ˜¯å°†æ¯ä¸€ä¸ªå­—ç¬¦éƒ½ä½œä¸ºä¸€ä¸ªtokenã€‚pythonä¸­çš„å­—ç¬¦ä¸²éƒ½æ˜¯Unicodeå­—ç¬¦ç»„æˆçš„åºåˆ—ï¼Œé€šè¿‡`ord(\"a\")`å¯ä»¥è·å–å­—ç¬¦å¯¹åº”çš„Unicodeç¼–ç ï¼Œåè¿‡æ¥ä½¿ç”¨`chr(97)`å¯ä»¥è·å¾—å¯¹åº”çš„å­—ç¬¦ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "print(ord(\"a\"))\n",
    "print(chr(97))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 108, 108, 111, 44, 32, 127757, 33, 32, 20320, 22909, 33]\n",
      "Hello, ğŸŒ! ä½ å¥½!\n"
     ]
    }
   ],
   "source": [
    "# æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿™ä¸ªæ˜ å°„å…³ç³»å¾—åˆ°ä¸€ä¸ªå­—ç¬¦çº§åˆ«çš„tokenizer\n",
    "class CharacterTokenizer():\n",
    "    def encode(self, string:str) -> list[int]:\n",
    "        return list(map(ord, string))\n",
    "    def decode(self, indices:list[int]) -> str:\n",
    "        return \"\".join(map(chr, indices))\n",
    "tokenizer = CharacterTokenizer()\n",
    "string = \"Hello, ğŸŒ! ä½ å¥½!\"\n",
    "indices = tokenizer.encode(string)\n",
    "print(indices)\n",
    "reconstructed_string = tokenizer.decode(indices)\n",
    "print(reconstructed_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™æ ·åšæ˜¯å¯è¡Œçš„ï¼Œä½†æ˜¯é—®é¢˜åœ¨äºUnicodeå®šä¹‰äº†è¶…è¿‡åäº”ä¸‡ä¸ªå­—ç¬¦ï¼Œç”±äºæ¯ä¸ªå­—ç¬¦éƒ½æœ‰ä¸€ä¸ªå”¯ä¸€çš„IDï¼Œè¿™æ ·å°±å¯¼è‡´äº†è¯æ±‡è¡¨æœ‰åå‡ ä¸‡ä¸ªæ˜ å°„ï¼Œéœ€è¦çš„å†…å­˜å¤ªå¤§ï¼›å¹¶ä¸”ç”±äºUnicodeä¸­å¤§éƒ¨åˆ†å­—ç¬¦å‡ºç°çš„é¢‘ç‡å…¶å®å¾ˆå°ï¼Œå› æ­¤è¿™ä¼šå¯¼è‡´æµªè´¹äº†å¾ˆå¤šç©ºé—´å­˜å‚¨ä½¿ç”¨é¢‘ç‡æä½çš„å­—ç¬¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5384615384615385"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_compression_ratio(string, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte-based Tokenization\n",
    "ä¸æŒ‰ç…§è¯æˆ–è€…å­—ç¬¦æ¥åˆ†ï¼Œè€Œæ˜¯å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºUTF-8ç¼–ç çš„å­—èŠ‚åºåˆ—ï¼Œæ¯ä¸€ä¸ªå­—èŠ‚å°±æ˜¯ä¸€ä¸ªtokenã€‚æ¯ä¸€ä¸ªå­—èŠ‚éƒ½æ˜¯ä¸€ä¸ª0-255çš„æ•´æ•°ï¼ŒUTF-8æ˜¯ä¸€ç§å˜é•¿çš„ç¼–ç æ–¹å¼ï¼Œå¯¹äºè‹±æ–‡åªéœ€è¦ä¸€ä¸ªå­—èŠ‚ï¼Œä½†æ˜¯å¯¹äºemojiæˆ–è€…æ±‰å­—åˆ™éœ€è¦3-4ä¸ªå­—èŠ‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes(\"a\", encoding=\"utf-8\") == b\"a\"\n",
    "bytes(\"ğŸŒ\", encoding=\"utf-8\") == b\"\\xf0\\x9f\\x8c\\x8d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 108, 108, 111, 44, 32, 240, 159, 140, 141, 33, 32, 228, 189, 160, 229, 165, 189, 33]\n",
      "Hello, ğŸŒ! ä½ å¥½!\n"
     ]
    }
   ],
   "source": [
    "class ByteTokenizer():\n",
    "    \"\"\"Represent a string as a sequence of bytes.\"\"\"\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        string_bytes = string.encode(\"utf-8\")  # å°†å­—ç¬¦ä¸²ç¼–ç ä¸ºUTF-8å­—èŠ‚æµ\n",
    "        indices = list(map(int, string_bytes))  # å°†æ¯ä¸ªå­—èŠ‚å–å‡ºæ¥ï¼Œè½¬æ¢ä¸ºæ•´æ•°åˆ—è¡¨\n",
    "        return indices\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        string_bytes = bytes(indices)  # å°†æ•´æ•°åˆ—è¡¨æ‰“åŒ…ä¸ºbytesç±»å‹\n",
    "        string = string_bytes.decode(\"utf-8\")  # è¿˜åŸä¸ºå­—ç¬¦ä¸²\n",
    "        return string\n",
    "tokenizer = ByteTokenizer()\n",
    "string = \"Hello, ğŸŒ! ä½ å¥½!\"\n",
    "indices = tokenizer.encode(string)\n",
    "print(indices)\n",
    "reconstructed_string = tokenizer.decode(indices)\n",
    "print(reconstructed_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™æ ·åšçš„å¥½å¤„å°±æ˜¯ä¸éœ€è¦æå‰è®­ç»ƒè¯è¡¨ï¼Œä½†æ˜¯è¿™æ ·çš„æ–¹æ³•å°†ä»»ä½•æ–‡æœ¬éƒ½æ‹†åˆ†çš„å¤ªç»†äº†ï¼Œç”±äºTransformerçš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯$\\mathcal{O}(N^2)$ï¼Œè¿™ä¼šå¯¼è‡´è®¡ç®—é‡è¿‡å¤§ï¼Œå› ä¸ºtokenå¤ªå¤šï¼Œåºåˆ—å¤ªé•¿ï¼Œå¹¶ä¸”è¿™ä¹Ÿä¼šå¯¼è‡´åç»­æ¨ç†è¿‡ç¨‹ä¹Ÿå¾ˆæ…¢ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_ratio = get_compression_ratio(string, indices)\n",
    "compression_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-based tokenization\n",
    "æŠŠä¸€å¥è¯æŒ‰ç…§è¯ä¸ºå•ä½æ¥åˆ†å‰²ï¼Œæ¯ä¸ªè¯ä½œä¸ºä¸€ä¸ªtoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'\", 'll', ' ', 'say', ' ', 'supercalifragilisticexpialidocious', '!']\n",
      "['I', \"'ll\", ' say', ' supercalifragilisticexpialidocious', '!']\n"
     ]
    }
   ],
   "source": [
    "import regex\n",
    "string = \"I'll say supercalifragilisticexpialidocious!\"\n",
    "segments = regex.findall(r\"\\w+|.\", string) # \\w+è¡¨ç¤ºè¿ç»­åŒ¹é…å­—æ¯ã€æ•°å­—æˆ–è€…ä¸‹åˆ’çº¿ |.è¡¨ç¤ºåŒ¹é…ä»»æ„å•ä¸ªå­—ç¬¦ï¼ˆä¿ç•™æ ‡ç‚¹ï¼‰\n",
    "print(segments)\n",
    "\n",
    "# GPT2ä½¿ç”¨äº†ä¸€ä¸ªæ›´ä¸ºä¸¥æ ¼çš„regex\n",
    "GPT2_TOKENIZER_REGEX = \\\n",
    "    r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "segments = regex.findall(GPT2_TOKENIZER_REGEX, string)\n",
    "print(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "éœ€è¦å¯¹æ¯ä¸ªåˆ†è¯æœ‰ä¸€ä¸ªå¯¹åº”çš„IDï¼Œå’Œå­—ç¬¦ä¸€æ ·ï¼Œå•è¯æ•°é‡å¾ˆå¤šï¼Œå¯èƒ½ä¼šå¯¼è‡´è¯è¡¨å¾ˆåºå¤§ï¼›å¹¶ä¸”å¾ˆå¤šç½•è§è¯å¯èƒ½åªå‡ºç°è¿‡ä¸€æ¬¡ï¼Œæ¨¡å‹å¾ˆéš¾å­¦åˆ°æœ‰ç”¨çš„è¡¨ç¤ºï¼›å¹¶ä¸”å¯¹äºè¯è¡¨ä¸­æ²¡æœ‰çš„è¯è¯­ï¼Œä¼šè¢«æ˜ å°„ä¸ºUNKï¼Œä¼šå¯¼è‡´å…¶å¤±å»åŸæœ‰çš„ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE(Byte Pair Encoding)\n",
    "åŸå§‹çš„ Byte Pair Encoding æ˜¯ä¸€ç§å‹ç¼©ç®—æ³•ï¼Œç”¨äºåå¤æ‰¾å‡ºæ–‡æœ¬ä¸­å‡ºç°æœ€é¢‘ç¹çš„ç›¸é‚»å­—ç¬¦å¯¹ï¼Œå¹¶æŠŠå®ƒä»¬åˆå¹¶æˆä¸€ä¸ªæ–°çš„ç¬¦å·ã€‚è¿™ç§æ–¹æ³•æ—¢èƒ½è¡¨ç¤ºå¸¸è§çš„è¯è¯­ï¼Œä¹Ÿèƒ½åˆ†è§£æœªçŸ¥çš„è¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@dataclass`æ˜¯python3.7å¼•å…¥çš„ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºç®€åŒ–ç±»çš„å®šä¹‰ï¼Œç‰¹åˆ«é€‚ç”¨äºæ•°æ®å¯¹è±¡çš„ç±»ï¼Œåªéœ€è¦å£°æ˜å­—æ®µï¼Œpythonå°±ä¼šè‡ªåŠ¨å¼•å…¥ä¸€å¤§å †å¸¸ç”¨çš„æ–¹æ³•ï¼Œæ¯”å¦‚è¯´`__init__`, `__eq__`ç­‰\n",
    "```python\n",
    "class Token:\n",
    "    def __init__(self, id, value):\n",
    "        self.id = id\n",
    "        self.value = value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Token(id={self.id}, value={self.value})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.id == other.id and self.value == other.value\n",
    "```\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Token:\n",
    "    id: int\n",
    "    value: str\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116, 104, 101, 32, 99, 97, 116, 32, 105, 110, 32, 116, 104, 101, 32, 104, 97, 116]\n",
      "the cat in the hat\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "# forzen=Trueè¡¨ç¤ºè¿™ä¸ªç±»çš„å®ä¾‹æ˜¯ä¸å¯å˜çš„ï¼Œå°±åƒå…ƒç»„ä¸€æ ·ï¼Œå¯ä»¥é˜²æ­¢è¢«æ›´æ”¹\n",
    "@dataclass(frozen=True) \n",
    "class BPETokenizerParams:\n",
    "    vocab: dict[int, bytes] # è¯è¡¨ï¼Œå°†æ•´æ•°å’Œå¯¹åº”çš„byteå­—èŠ‚ä¸²å¯¹åº”èµ·æ¥\n",
    "    merges: dict[tuple[int, int], int] # BPEè®­ç»ƒè¿‡ç¨‹ä¸­å­¦åˆ°çš„åˆå¹¶æ³•åˆ™ï¼Œå°†ä¸€ä¸ªtokenå¯¹(index1,index2)åˆå¹¶ä¸ºä¸€ä¸ªé‡æ–°åˆ†é…çš„token ID(new_index)\n",
    "                                       # ä¾‹å¦‚ merges[(72, 105)] = 256\n",
    "\n",
    "def merge(indices:list[int], pair:tuple[int, int], new_index:int) -> list[int]:\n",
    "    new_indices = []\n",
    "    i = 0\n",
    "    while i < len(indices):\n",
    "        if i + 1 < len(indices) and indices[i] == pair[0] and indices[i] == pair[1]:\n",
    "            new_indices.append(new_index)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_indices.append(indices[i])\n",
    "            i += 1\n",
    "    return new_indices\n",
    "\n",
    "\n",
    "def train_bpe(string: str, num_merges: int) -> BPETokenizerParams:\n",
    "    indices = list(map(int, string.encode(\"utf-8\")))\n",
    "    merges: dict[tuple[int, int], int] = {}\n",
    "    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)} # åˆå§‹åŒ–è¯è¡¨\n",
    "    for i in range(num_merges):\n",
    "        counts = defaultdict(int) # åˆ›å»ºä¸€ä¸ªdefaultdictå¯¹è±¡ï¼Œå¦‚æœè®¿é—®åˆ°ä¸€ä¸ªä¸å­˜åœ¨çš„å€¼ï¼Œä¸ä¼šè·‘å‡ºKeyErrorï¼Œè€Œæ˜¯ä¼šå°†è¿™ä¸ªæ–°é”®åˆå§‹åŒ–ä¸ºint()çš„è¿”å›å€¼\n",
    "        for index1, index2 in zip(indices, indices[1:]): # zipä¼šå°†å¤šä¸ªåºåˆ—çš„å¯¹åº”å…ƒç´ æ‰“åŒ…ä¸ºä¸€ä¸ªä¸ªå…ƒç»„\n",
    "            counts[(index1, index2)] += 1 # æœ€åä¼šå­˜å‚¨å­—ç¬¦ä¸²ä¸­æ¯ä¸ªç›¸é‚»å­—èŠ‚å¯¹å‡ºç°çš„æ¬¡æ•°\n",
    "        pair = max(counts, key=counts.get) # é€‰å‡ºé¢‘ç‡æœ€é«˜çš„pair\n",
    "        index1, index2 = pair\n",
    "        new_index = 256 + i\n",
    "        merges[(index1, index2)] = new_index\n",
    "        vocab[new_index] = vocab[index1] + vocab[index2]\n",
    "        indices = merge(indices, pair, new_index)\n",
    "    return BPETokenizerParams(vocab, merges)\n",
    "\n",
    "class BPETokenizer():\n",
    "    def __init__(self, params:BPETokenizerParams):\n",
    "        self.params = params\n",
    "    def encode(self, string:str) -> list[int]:\n",
    "        indices = list(map(int, string.encode(\"utf-8\"))) \n",
    "        for pair, new_index in self.params.merges.items():\n",
    "            indices = merge(indices, pair, new_index)\n",
    "        return indices\n",
    "    def decode(self, indices:list[int]) -> str:\n",
    "        # map(self.params.vocab.get, indices) å°†indicesä¸­çš„æ¯ä¸€ä¸ªIDæ˜ å°„ä¸ºvocabä¸­å¯¹åº”çš„byte\n",
    "        # bytes_list = [self.params.vocab.get(index) for index in indices]\n",
    "        bytes_list = list(map(self.params.vocab.get, indices)) \n",
    "        string = b\"\".join(bytes_list).decode(\"utf-8\")\n",
    "        return string\n",
    "\n",
    "string = \"the cat in the hat\"\n",
    "params = train_bpe(string, 3)\n",
    "tokenizer = BPETokenizer(params)\n",
    "indices = tokenizer.encode(string)\n",
    "print(indices)\n",
    "reconstructed_string = tokenizer.decode(indices)\n",
    "print(reconstructed_string)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(116, 104): 258}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'A'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'\\t'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = \"A\".encode(\"utf-8\") \n",
    "print(b) # è™½ç„¶æ‰“å°å‡ºæ¥æ˜¯b'A'ä½†æ˜¯å®é™…ä¸Šé‡Œé¢å­˜çš„æ˜¯ä¸€ä¸ª0-255çš„æ•´æ•°\n",
    "bytes([9]) # åˆ›å»ºäº†ä¸€ä¸ªé•¿åº¦ä¸º1çš„å­—èŠ‚ä¸²ï¼Œå¿…é¡»ä¼ å…¥çš„æ˜¯ä¸€ä¸ªå¯è¿­ä»£çš„å¯¹è±¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`str.encode(\"utf-8\")`å°†å­—ç¬¦ä¸²å˜ä¸ºå­—èŠ‚ä¸²\n",
    "`bytes.decode(\"utf-8\")`å°†å­—èŠ‚ä¸²å˜ä¸ºå­—ç¬¦ä¸²\n",
    "\n",
    "åœ¨Python ä¸­ï¼Œ`b''` è¡¨ç¤ºä¸€ä¸ªå­—èŠ‚ä¸²(bytes object)ï¼Œå…¶ä¸­å­˜å‚¨çš„æ¯ä¸ªå­—ç¬¦éƒ½æ˜¯ä¸€ä¸ªbyteã€‚`list(b\"Hi\")`å®é™…ä¸Šå°±æ˜¯å§å­—ç¬¦ä¸²è½¬å˜ä¸ºä¸€ä¸ªå­—èŠ‚å€¼åˆ—è¡¨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
