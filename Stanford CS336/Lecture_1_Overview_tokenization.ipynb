{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Examples\n",
    "通过这个网站[Tiktokenizer](https://tiktokenizer.vercel.app/?encoder=gpt2)可以感受一下分词器是如何运作的。\n",
    "```\n",
    "<|im_start|>system<|im_sep|>You are a helpful assistant<|im_end|>\n",
    "<|im_start|>user<|im_sep|>I am studying CS336<|im_end|><|im_start|>assistant<|im_sep|>\n",
    "```\n",
    "这一段内容是一个示例，是我们和人工智能之间的对话的系统格式，我发现一个有趣的现象，如果直接将上面这段内容复制粘贴到ChatGPT中，并且让它给你解释一下，它只会认为你是一个正在学习CS336的同学，但是如果在外面加一个引号然后引上这段内容，ChatGPT才能为你解释这段内容。这也正好验证了这是人类和人工智能之间的对话的例子。\n",
    "\n",
    "通过这个例子观察到的**现象**：\n",
    "- 在英文中一个单词前面的空格会和它一起组成一个token；\n",
    "- 相同的单词出现在句首和句中，token不一定相同，例如\"hello\"是句首的token，\" hello\"是句中token（注意有个空格），这也就会导致最后得到的整数不一样；\n",
    "- 一个数字不会被当作一个整体，例如2025可能被分为两组来理解（202、5）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/gyr04/anaconda3/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/gyr04/anaconda3/lib/python3.10/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/gyr04/anaconda3/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/gyr04/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gyr04/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/gyr04/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gyr04/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [15496, 11, 12520, 234, 235, 0, 220, 19526, 254, 25001, 121, 0]\n",
      "Decoded Text: Hello, 🌍! 你好!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"Hello, 🌍! 你好!\"\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**compression ratio**是压缩率，指的是原始的字符串长度（以UTF-8编码）与分词后的token数量的比值。\n",
    "- 比值越大代表每个token能代表的字节越多，也就是压缩效果好；\n",
    "- 比值越小代表分词更细碎，压缩效果差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6666666666666667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_compression_ratio(string: str, indices: list[int]) -> float:\n",
    "    \"\"\"Given `string` that has been tokenized into `indices`, .\"\"\"\n",
    "    num_bytes = len(bytes(string, encoding=\"utf-8\"))  # @inspect num_bytes\n",
    "    num_tokens = len(indices)                       # @inspect num_tokens\n",
    "    return num_bytes / num_tokens\n",
    "get_compression_ratio(text, token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-based tokenization\n",
    "字符级分词器也就是将每一个字符都作为一个token。python中的字符串都是Unicode字符组成的序列，通过`ord(\"a\")`可以获取字符对应的Unicode编码，反过来使用`chr(97)`可以获得对应的字符："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "print(ord(\"a\"))\n",
    "print(chr(97))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 108, 108, 111, 44, 32, 127757, 33, 32, 20320, 22909, 33]\n",
      "Hello, 🌍! 你好!\n"
     ]
    }
   ],
   "source": [
    "# 我们可以通过这个映射关系得到一个字符级别的tokenizer\n",
    "class CharacterTokenizer():\n",
    "    def encode(self, string:str) -> list[int]:\n",
    "        return list(map(ord, string))\n",
    "    def decode(self, indices:list[int]) -> str:\n",
    "        return \"\".join(map(chr, indices))\n",
    "tokenizer = CharacterTokenizer()\n",
    "string = \"Hello, 🌍! 你好!\"\n",
    "indices = tokenizer.encode(string)\n",
    "print(indices)\n",
    "reconstructed_string = tokenizer.decode(indices)\n",
    "print(reconstructed_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样做是可行的，但是问题在于Unicode定义了超过十五万个字符，由于每个字符都有一个唯一的ID，这样就导致了词汇表有十几万个映射，需要的内存太大；并且由于Unicode中大部分字符出现的频率其实很小，因此这会导致浪费了很多空间存储使用频率极低的字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5384615384615385"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_compression_ratio(string, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte-based Tokenization\n",
    "不按照词或者字符来分，而是将字符串转换为UTF-8编码的字节序列，每一个字节就是一个token。每一个字节都是一个0-255的整数，UTF-8是一种变长的编码方式，对于英文只需要一个字节，但是对于emoji或者汉字则需要3-4个字节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes(\"a\", encoding=\"utf-8\") == b\"a\"\n",
    "bytes(\"🌍\", encoding=\"utf-8\") == b\"\\xf0\\x9f\\x8c\\x8d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 108, 108, 111, 44, 32, 240, 159, 140, 141, 33, 32, 228, 189, 160, 229, 165, 189, 33]\n",
      "Hello, 🌍! 你好!\n"
     ]
    }
   ],
   "source": [
    "class ByteTokenizer():\n",
    "    \"\"\"Represent a string as a sequence of bytes.\"\"\"\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        string_bytes = string.encode(\"utf-8\")  # 将字符串编码为UTF-8字节流\n",
    "        indices = list(map(int, string_bytes))  # 将每个字节取出来，转换为整数列表\n",
    "        return indices\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        string_bytes = bytes(indices)  # 将整数列表打包为bytes类型\n",
    "        string = string_bytes.decode(\"utf-8\")  # 还原为字符串\n",
    "        return string\n",
    "tokenizer = ByteTokenizer()\n",
    "string = \"Hello, 🌍! 你好!\"\n",
    "indices = tokenizer.encode(string)\n",
    "print(indices)\n",
    "reconstructed_string = tokenizer.decode(indices)\n",
    "print(reconstructed_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样做的好处就是不需要提前训练词表，但是这样的方法将任何文本都拆分的太细了，由于Transformer的注意力机制是$\\mathcal{O}(N^2)$，这会导致计算量过大，因为token太多，序列太长，并且这也会导致后续推理过程也很慢。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_ratio = get_compression_ratio(string, indices)\n",
    "compression_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-based tokenization\n",
    "把一句话按照词为单位来分割，每个词作为一个token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'\", 'll', ' ', 'say', ' ', 'supercalifragilisticexpialidocious', '!']\n",
      "['I', \"'ll\", ' say', ' supercalifragilisticexpialidocious', '!']\n"
     ]
    }
   ],
   "source": [
    "import regex\n",
    "string = \"I'll say supercalifragilisticexpialidocious!\"\n",
    "segments = regex.findall(r\"\\w+|.\", string) # \\w+表示连续匹配字母、数字或者下划线 |.表示匹配任意单个字符（保留标点）\n",
    "print(segments)\n",
    "\n",
    "# GPT2使用了一个更为严格的regex\n",
    "GPT2_TOKENIZER_REGEX = \\\n",
    "    r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "segments = regex.findall(GPT2_TOKENIZER_REGEX, string)\n",
    "print(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要对每个分词有一个对应的ID，和字符一样，单词数量很多，可能会导致词表很庞大；并且很多罕见词可能只出现过一次，模型很难学到有用的表示；并且对于词表中没有的词语，会被映射为UNK，会导致其失去原有的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE(Byte Pair Encoding)\n",
    "原始的 Byte Pair Encoding 是一种压缩算法，用于反复找出文本中出现最频繁的相邻字符对，并把它们合并成一个新的符号。这种方法既能表示常见的词语，也能分解未知的词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@dataclass`是python3.7引入的一个装饰器，用于简化类的定义，特别适用于数据对象的类，只需要声明字段，python就会自动引入一大堆常用的方法，比如说`__init__`, `__eq__`等\n",
    "```python\n",
    "class Token:\n",
    "    def __init__(self, id, value):\n",
    "        self.id = id\n",
    "        self.value = value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Token(id={self.id}, value={self.value})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.id == other.id and self.value == other.value\n",
    "```\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Token:\n",
    "    id: int\n",
    "    value: str\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116, 104, 101, 32, 99, 97, 116, 32, 105, 110, 32, 116, 104, 101, 32, 104, 97, 116]\n",
      "the cat in the hat\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "# forzen=True表示这个类的实例是不可变的，就像元组一样，可以防止被更改\n",
    "@dataclass(frozen=True) \n",
    "class BPETokenizerParams:\n",
    "    vocab: dict[int, bytes] # 词表，将整数和对应的byte字节串对应起来\n",
    "    merges: dict[tuple[int, int], int] # BPE训练过程中学到的合并法则，将一个token对(index1,index2)合并为一个重新分配的token ID(new_index)\n",
    "                                       # 例如 merges[(72, 105)] = 256\n",
    "\n",
    "def merge(indices:list[int], pair:tuple[int, int], new_index:int) -> list[int]:\n",
    "    new_indices = []\n",
    "    i = 0\n",
    "    while i < len(indices):\n",
    "        if i + 1 < len(indices) and indices[i] == pair[0] and indices[i] == pair[1]:\n",
    "            new_indices.append(new_index)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_indices.append(indices[i])\n",
    "            i += 1\n",
    "    return new_indices\n",
    "\n",
    "\n",
    "def train_bpe(string: str, num_merges: int) -> BPETokenizerParams:\n",
    "    indices = list(map(int, string.encode(\"utf-8\")))\n",
    "    merges: dict[tuple[int, int], int] = {}\n",
    "    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)} # 初始化词表\n",
    "    for i in range(num_merges):\n",
    "        counts = defaultdict(int) # 创建一个defaultdict对象，如果访问到一个不存在的值，不会跑出KeyError，而是会将这个新键初始化为int()的返回值\n",
    "        for index1, index2 in zip(indices, indices[1:]): # zip会将多个序列的对应元素打包为一个个元组\n",
    "            counts[(index1, index2)] += 1 # 最后会存储字符串中每个相邻字节对出现的次数\n",
    "        pair = max(counts, key=counts.get) # 选出频率最高的pair\n",
    "        index1, index2 = pair\n",
    "        new_index = 256 + i\n",
    "        merges[(index1, index2)] = new_index\n",
    "        vocab[new_index] = vocab[index1] + vocab[index2]\n",
    "        indices = merge(indices, pair, new_index)\n",
    "    return BPETokenizerParams(vocab, merges)\n",
    "\n",
    "class BPETokenizer():\n",
    "    def __init__(self, params:BPETokenizerParams):\n",
    "        self.params = params\n",
    "    def encode(self, string:str) -> list[int]:\n",
    "        indices = list(map(int, string.encode(\"utf-8\"))) \n",
    "        for pair, new_index in self.params.merges.items():\n",
    "            indices = merge(indices, pair, new_index)\n",
    "        return indices\n",
    "    def decode(self, indices:list[int]) -> str:\n",
    "        # map(self.params.vocab.get, indices) 将indices中的每一个ID映射为vocab中对应的byte\n",
    "        # bytes_list = [self.params.vocab.get(index) for index in indices]\n",
    "        bytes_list = list(map(self.params.vocab.get, indices)) \n",
    "        string = b\"\".join(bytes_list).decode(\"utf-8\")\n",
    "        return string\n",
    "\n",
    "string = \"the cat in the hat\"\n",
    "params = train_bpe(string, 3)\n",
    "tokenizer = BPETokenizer(params)\n",
    "indices = tokenizer.encode(string)\n",
    "print(indices)\n",
    "reconstructed_string = tokenizer.decode(indices)\n",
    "print(reconstructed_string)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(116, 104): 258}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'A'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'\\t'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = \"A\".encode(\"utf-8\") \n",
    "print(b) # 虽然打印出来是b'A'但是实际上里面存的是一个0-255的整数\n",
    "bytes([9]) # 创建了一个长度为1的字节串，必须传入的是一个可迭代的对象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`str.encode(\"utf-8\")`将字符串变为字节串\n",
    "`bytes.decode(\"utf-8\")`将字节串变为字符串\n",
    "\n",
    "在Python 中，`b''` 表示一个字节串(bytes object)，其中存储的每个字符都是一个byte。`list(b\"Hi\")`实际上就是吧字符串转变为一个字节值列表"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
